[
["index.html", "Nursing School Research Analytics and Computation Report Chapter 1 Executive Summary and Recommendations 1.1 Participants 1.2 Motivations 1.3 Findings / Initial Recommendations:", " Nursing School Research Analytics and Computation Report Steve Pittard 2018-12-07 Chapter 1 Executive Summary and Recommendations Conducting these interviews was a smooth process as those contacted were very forthcoming with helpful responses that were very consistent across the topic areas described in this report. Deviations of opinions did sometimes occur along the lines of junior and senior faculty although all of the senior faculty with whom I spoke were keenly aware of the difficulties facing junior faculty and appeared to be sympathetic. Everyone was collegial and supplied viewpoints that will benefit the School as a whole. The concerns expressed were entirely relevant to improving research processes and it’s clear that even though there are some challenges, people are nonetheless working very hard to get things done. The School of Nursing has definite strengths in its clinical programs combined with accomplished and talented faculty. It remains a priority to continue to apply for data-based grants and to this end the School must be able to point to official and strong support for analytics and computation within proposals to be competitive. 1.1 Participants The participants in the interview process were: Linda McCauley, Anne Dunlop, Carolyn Clevenger, Jessica Wells, Irene Yang, Rebecca Mitchell, Jinbing Bai, Erin Ferratti, Betsy Corwin, Nicole Carslon, Despina Tsementzi, Anna Knight, Deborah Bruner, Mary Gullate, Melinda Higgins, and Jason Atkins. I also had a discussion with Karl Moran at Microbiome Insights since they performed work for Irene Yang. I also contacted Ethenia Why at the Biostatistics Core at Hopkins for comments on their role in Microbiome analysis. I’ve also got an inquiry to Steven Gill at the University of Rochester Microbiome Core. I’ve had conversations with Madhusmita Behera who is the Director of Informatics at Winship Cancer Institute which is also working towards a usable solution for Data Management and Computation. 1.2 Motivations The original scope of this report was initially focused on addressing the problems associated with managing data and omics-based research though it rapidly became apparent that those interviewed were equally as concerned about addressing and improving relationships with Emory Core facilities. To a lesser extent (although still important) there us frustration associated with being able to conveniently obtain data from the Clinical Data Warehouse and the perceived lack of customer focus exhibited by the health services research personnel. 1.2.1 The “Omics” Problem The magnitude and highly dynamic nature of omics-based projects can lead to interesting explorations though ultimately the research should lead to publications in high impact journals and successful grant applications. These goals, however, can be frustrated by uncertainty about how best to manage data, which analytic techniques are most appropriate, where the computation should be performed, and how to effectively determine the biological relevance of the result. This can also be particularly challenging for doctoral students who can sometimes receive conflicting advice on how to structure omics-based thesis projects. Creating smooth workflows using omics-data is essential to progress and publications thus strategies must be developed to effectively deal with this situation. At a functional level, the development and implementation of a common logistical workflow would be helpful to guide doctoral students and faculty throughout the project life cycle. This is particularly true in the planning phase wherein documenting required personnel, appropriate percent effort, and anticipated service core involvement becomes essential to the outcome of having a defensible, publishable result that is accomplished according to known standards for a given Omics type (e.g. microbiome, metabolomics). 1.2.2 Communication With Cores Nursing School investigators engage Emory Core facilities to obtain experimental and analytic services not commonly available or affordable within individual labs. The expectation is that these engagements will advance the quality and speed of research and that results emanating from these centers will be informative, accurate, and reflect a high level of consideration for individual project goals. However, a majority of those interviewed have concerns about the dynamics and products being returned from the Cores. (A notable exception is the Lipidomics Core). The concerns relate primarily to up front project design, the interpetability of the results, as well as the overall level of attention being applied throughout the life cycle of the project. The impression remains that many current omics-based projects wind up being unnecessarily reactive and delayed since investigators (and students) have ongoing questions after receiving results although from the Core point of view the engagement has concluded. Their goal is to execute a discrete, high-quality experiment, thus an ongoing bioinformatics consultation might not be possible without additional financial support. Closing the gap between user expectations and the typical Core product is important as is having reasonable ongoing access to expertise (internal or otherwise). 1.2.3 Data Access and Collaboration A more fundamental problem exists in having convenient access to data as it makes its way through the analytics pipeline and in understanding what transformations have been applied (or not) to a candidate result. A solution such as RedCap is important though it can host only the more structured part of the project data whereas the associated omics data sets typically wind up on an Emory Box account that cannot be computed against. This leads to data being distributed to colleagues resulting in multiple copies that are transformed according to the needs of whomever is currently working on it (statistician, biologist, bioinformatician). Unfortunately, the transformed data and results (as well as the code used to produce it) are rarely reintegrated back into a repository for common reference. This is understandable as there is no common platform upon which to perform the work which could also serve as a sharing / collaboration environment. It is generally agreed that maintaining projects on laptops is a problem though it is a popular approach. However, not all PIs feel comfortable sharing data especially in scenarios where the data has been difficult to obtain. It’s agreed that research reproducibility is important but there are sensitivities to having data commonly available during the project. (Appropriate security approaches can help here). Independently of these concerns, the funding agencies, (via concepts such as the “FAIR Guiding Principles” for data management and stewardship), are very much concerned with the “long-term care” of valuable digital assets, thus the appropriate annotation and archival of data is an obligation. 1.3 Findings / Initial Recommendations: Note that these represent suggested starting points which can flexibly be implemented. Most, if not all, of these recommendations are based on (or are motivated by) suggestions from faculty who have attempted to address these problems in various ways including leveraging personal, professional, and external relationships. Further discussion is warranted particularly on items relating to the formation of a central resource (1.2.2) and the licensing of a data management and computation framework (1.2.1). The following statements were derived from the more detailed sections (2-6) of this document which in turn were based on the faculty interviews. Data management is a very important aspect of analysis and research although the current practices for supporting these activities are less than ideal. Intermediate results and data transformations are not maintained alongside original data which impairs reproducibility. The data residing in Emory Box cannot be computed against and must be moved to individual computers (sometimes several) for downstream analysis. A framework is needed to maintain experimental data, code, and notebooks. The School should consider use of a tool such as DNANexus or Seven Bridges Genomics either of which would provide comprehensive support for managing sequencing-based projects, analytics, as well as the convenient addition of self-developed pipelines. This would address a number of concerns of having “everything in one place” (to the extent that it is possible) along with computational results in a format that would enable reproducibility. Additionally, a solution such as DNANexus would facilitate the integration of genomic data with clinical and other phenotypic data in a secure and compliant environment. While RedCap is useful for maintaining study information, being able to link in sequencing and sample information can be challenging. For a more detailed analysis of this topic please consult section 4) Interaction with Cores. Nursing-Centric Analytics ‘Omics’ Resource Availability of professionals who are well-versed in analytics, statistics, and Omics are critical to the research mission of the School. To this end, the formation of a nursing-centric service facility primarily for the School is recommended. This entity would provide services for analytics, computing, and interactions with sequencing facilities (where desired) and generally exist for the benefit of investigators. Whether this service center is an “official university core” or a formalized group is up for debate although the feedback on this topic is clear in that the group should prioritize Nursing School interests while also being available for salary support via external projects albeit secondarily. Such a group would not replace or diminish the relationships with existing University Cores but rather improve the working dynamic since Nursing investigators would have an advocate acting on their behalf. Duties would involve assistance with data management, cleaning, preliminary analysis, and ongoing participation in projects. Organizing training and providing orientation to the framework mentioned in item #1 would also be on the menu of services. It is important to note that such a resource could be referenced in applications for grants involving large scale processing. This would be evidence of a formal institutional commitment to Nursing analytics. Opinions vary as to the exact composition of this group and/or if it should draw from existing personnel (who aren’t already fully funded) but this is a strongly supported idea among those interviewed. For a more detailed analysis of this topic please consult section 4) Interaction with Cores. Improved Project Logistics Researchers would like more detailed upfront estimates of statistics, analytics, and computation costs to better project cost of staff involvement. Not knowing how much (in time, money, and personnel) the key aspects of a project will “really” involve is a concern. To this end, having access to a person familiar with the aforementioned areas to help with up front assessment would be very useful. This could be implemented separately of option #2 though if the resource in option #2 did exist then this person could also be part of that that. The overall idea is to have accurate and reliable estimates of sequencing, analytics, and computation work. Some junior faculty feel that they cannot fully access personnel due to lack of funding or because the more experienced personnel are already mostly engaged on other projects. In general, having a standard logistical workflow that specifies who to involve, when, and to what extent is needed. This includes collaborating domain expertise (statistics and bioinformatics personnel) as well as key Core facilities - internal or external to Emory. Computation remains a challenge in that the School does not currently “own” any significant computational resources which has led researchers to leverage personal and external professional relationships to accomplish work. This approach 1) does not scale well, 2) works against the idea of reproducible research, and 3) impairs collaborative ability. While Amazon Web Services is a possibility, it requires significant training and assistance in setting up, maintaining, and breaking down projects which makes it unlikely that individual investigators could conveniently use such a service in absence of ongoing help. The university is, at least at this point, providing only basic forms of onboarding support and the majority of support from Amazon relates primarily to demonstrations of various Amazon services that, while helpful, do not involve detailed “dives” into omics-based work. While learning more about Amazon is important, the School should consider the use of a tool such as DNANexus or Seven Bridges Genomics that overlays Amazon AWS while providing intuitive support for data management and computation. Solving the data management and computation problem, in addition to being able to easily execute and add pipelines, is a more direct route to productivity than building out infrastructure in the cloud or locally. A secondary approach might involve partnering with the Department of Biomedical Informatics which maintains a sophisticated, well-functioning, well-supported computational cluster dedicated to aggressive research computing. This would have to be arranged but none of the projects currently being considered within the School of Nursing would represent a challenge to that environment. BMI would be an appropriate partner because research computing is “baked-in” to their mission and is thus seen as an essential service for researchers. I discussed a potential relationship with Jim Kinney (BMI System Lead) earlier this year but would need to renew those talks. Alternatives to Local Sample Processing The School should develop relationships with trusted external scientific resource providers to process urgent requests for sample processing and analysis consultation. This should also include training resources which could be accomplished virtually, on-premise (for larger groups), or remotely. Not every project will require this but there should be a reliable outlet for “blocked” projects or those requiring a rapid turnaround. In particular, we should organize training to learn “best practices” in the processing and analysis of Microbiome and Metabolomics. At least one faculty member has used an external lab for Microbiome and another is contemplating the re-sequencing of a large number of samples via the Ravel Lab at the Institute of Genome Sciences to obtain domain expertise starting at the sequencing level. Improving general software literacy is very important as is being able to import/transform data, accomplish analytic tasks, create plots, query databases, and create digital assets will facilitate publication and grant submissions. Many faculty pursue self-education but are interested in becoming much more self-sufficient with open source tools and the UNIX command line. This can be accomplished via participation in the Big Data course or more specific classes. However, one excellent (and cheap) resource is to arrange for an onsite Software Carpentry session which is a 1 to 3 day workshop devoted to teaching basic lab skills for research computing. These sessions are professionally taught and include hands on labs to learn the UNIX command line, R and Python Programming, Git, SQL and Databases. These are targeted to the novice but would also serve to reinforce skills. The material is open source and maintained on GitHub so we could possibly offer the course material with local teaching resources. The following graph illustrates the motivation levels for various software and command line topics after taking a Core level Software Carpentry class. Student Recruitment - Deliberate efforts should be made to recruit graduate students with ability and/or interest in data manipulation and analysis since the nature of research within the school will require such a background. While no one believes that using only student “labor” to accomplish projects is reasonable, there is a goal of having software literate students who can “dive in” to projects. However, the School Dean did express a specific concern that it can be challenging for doctoral students who can sometimes receive conflicting advice on how to structure omics-based thesis projects. So while having software savvy students is a plus, they will still require guidance in the selection and execution of their thesis work which in turn assumes the existence of a knowledgeable community. The following sections (2-6) represent the interview summary information which was used as a basis for the above recommendations. Each section takes a deeper dive into the area under consideration. "],
["data-management.html", "Chapter 2 Data Management 2.1 Data Hosting 2.2 Source Code Maintenance 2.3 Notebooks and Reproducible Research", " Chapter 2 Data Management Summary: The organization, preservation, and sharing of data are very important aspects of analysis and research although the current methods for doing this are not satisfactory. Intermediate results and data transformations are not maintained alongside original data which impairs reproducibility. A framework(s) is needed to maintain experimental data, code, and notebooks. 2.1 Data Hosting The primary method of managing data involves the use of Emory Box (https://emory.account.box.com/login in conjunction with RedCap https://www.project-redcap.org/ although no one interviewed believes Box to be ideal or particularly desirable outside of its approved use for hosting health data. The data being hosted on Box is typically distributed to collaborators and analysts who in turn maintain personally transformed versions of the data and associated intermediate results. It is rare that those results and transformed datasets are re-integrated back alongside the original data in a manner that facilitates reproducibility. While study data is being maintained in RedCap there are an increasing number of associated experimental data types that aren’t appropriate for tracking in RedCap thus they need to be maintained separately on Box. However, the Box resource cannot be computed against. Some faculty do use R Notebooks for reproducibility but point to the problem of keeping track of results generated by others who might “touch”&quot; the project and preserve their own scripts and transformed data on their laptop. Given that shuffling data around is largely a manual process, trying to “work back” from results to the original data is challenging. 2.2 Source Code Maintenance Many of the reference software pipelines for Microbiome analysis (as well as other data types) are obtained from literature references, training, and/or vignettes that might be supplied with a given software package. It is expected that changes will be made to scripts simply to accommodate local data sources which might very well lead to questions and interactions with package authors, statisticians, and more generally anyone who might be able to help the research bypass roadblocks. Thus, the state of a script or code can vary greatly at any time depending on where in the process things are. These scripts are almost always maintained on personal laptops or desktop computers although at least one person uses Git and GitHub to archive scripts. Even so, the scripts are typically highly customized for a specific project. Also, GitHub is for maintaining code changes and not data thus it is inappropriate to host data and results (intermediate or final) along with the code. It’s also true that GitHub exists to encourage collaboration and co-development though not everyone who would use Git or GitHub feels that the code is in a “good enough” state to push to a repository for general use and inspection. 2.3 Notebooks and Reproducible Research Some faculty are using R Notebooks to capture a narrative alongside the code used to generate results. These notebooks are prime candidates for registration on GitHub as it allows others to benefit from the work and permits easy retrieval of previous versions. Of course, some do not wish to share their code for various reasons such as that is in development. Another reason is that the end result is simply a minor variation of a published pipeline (e.g. Qiime2 or dada2) in response to specific sequencing problems or situations that might not relate to another PIs project. Some labs have in fact maintained scripts in GitHub or on the Costas website at http://enve-omics.ce.gatech.edu/ although the practice is not prevalent. Reproducing research is major concern that has been placed aside in favor of simply being able to get analysis accomplished (which itself has been a challenge). In short “the data can’t grow roots anywhere” so being able to integrate all intermediate results, let alone track the provenance and chain of custody thereof, has been very challenging outside of the simplest of projects. A larger issue exists in knowing the “best practices” associated with creating reproducible resources. Some faculty do know about “notebooks” in R and Python and see the value in using them although integrating a personal notebook with that of an analyst or collaborator remains a challenge. Keeping copies of “cleaning” scripts in addition to analysis pipelines is a priority though concern remains at being able to reproduce results if and when key personnel leave the department. "],
["software-literacy.html", "Chapter 3 Software Literacy 3.1 Concerns About Falling Behind 3.2 Workshops 3.3 SAS/SPSS vs Open Source Tools 3.4 Attracting Software Literate Students", " Chapter 3 Software Literacy Summary: Having software literacy is essential to research success. Being able to import/transform data, accomplish analytic tasks, create plots, query databases, and create digital assets will facilitate publication and grant submissions. Many faculty would like to become more self-sufficient. 3.1 Concerns About Falling Behind Some faculty have improved their knowledge via self-education or participating in the Big Data class though most want more experience and are frustrated with not being able to fully comprehend the code in published pipelines and/or how one might modify or extend the code to pursue ad-hoc analysis paths. Consequently, they feel “locked in” and limited by their relative lack of the UNIX command line and general programming knowledge and might not then feel confident enough to modify code for fear of “breaking” something. This is especially concerning given that the junior faculty know that they need solid preliminary data before they can apply for an R series grant. At this point being able to conduct and understand analytics results is a barrier to progress. 3.2 Workshops A popular approach to learning coding in addition to analysis are domain specific workshops such as those offered for Qiime, dada2, and the 5-day training at UAB which has apparently now been reduced to a half day. However, there is wide variation reported in the quality and applicability of general workshops with some being somewhat helpful but not necessarily representative of “real world” data. This was a common complaint especially relative to Microbiome projects wherein the sequencing data coming back from the sequencing center was causing problems with the standard published dada2 pipeline. The UAB workshop was singled out as being particularly helpful since it began with sequencing considerations all the way to full on analysis and result reporting. It has been suggested that 1) The School identify quality training resources and 2) create relationships with them to arrange for on-site or remote training. The Microbiome Insights group Lab (microbiomeinsights.com) will offer onsite or virtual training sessions on demand for individuals or groups although more work needs to be done to identify specific topics of interest. 3.3 SAS/SPSS vs Open Source Tools Some faculty have graduate training and exposure to SAS and SPSS and remain attracted to the integrated nature of those tools but realize that R and Python are the more relevant frameworks for the Omics work at hand. To this end, Coursera and Edx courses have been useful to pick up targeted knowledge though it is generally agreed that project driven coding is the best for learning and reinforcing skills. Leveraging Software Carpentry https://software-carpentry.org/lessons style courses is also a reasonable possibility since they offer 1 to 3 days workshops desgined to impart basic computation skills for researchers. One faculty member suggested making the Big Data class a two-semester sequence with one class being devoted to the key aspects of the language which, in combination with the project-based course, would represent a powerful educational offering. However, some faculty might not wish to devote that much time to this effort though willingly concede that it would be helpful. 3.4 Attracting Software Literate Students There is an associated interest in attracting students who possess programming skills or are at the very least enthusiastic about a learning path that involves programming. This is both to facilitate research and to make the student experience more productive since analysis of experimental data types is likely to be a major theme in most thesis projects. It is important to note that such students are not being seen as a substitute for more formal types of support though as part of their educational experience they could work in an internal service center or core to interact with experienced professionals. Again, this is not intended to be a roundabout method of getting research projects accomplished but an attempt at accelerating student knowledge of “real world” analysis applications. "],
["interaction-with-cores.html", "Chapter 4 Interaction with Cores 4.1 Expectations of Cores 4.2 Understanding Results 4.3 Need for More Consultation 4.4 Possible Solutions", " Chapter 4 Interaction with Cores Summary: Satisfaction levels with Emory Cores could be better. Researchers want more up front support in addition to back end consultation. Faculty also understand that they have a responsibility to acquire the requisite knowledge to speak intelligently about the project but until they “get there” they will need help. 4.1 Expectations of Cores Expectations vary as to what Emory Cores should be providing customers with some faculty wanting significantly more up front and post-result assistance on how to accomplish variations in analysis. Communication difficulties with the Sequencing Core were mentioned by a majority of those interviewed with the conclusion that perhaps seeking an external partner with expertise in Microbiome and Metabolomics would be something to consider. One faculty member has used the services of an external provider mostly for analytics and was happy with the result though follow-up dialogue was required to clarify conclusions. Another faculty member retained a postdoc to actually do the sequencing and analysis which resulted in a lean, efficient project since it was all done under the supervision and involvement of one person. It was acknowledged that this situation is rare and probably not something that could be reproduced and scaled for use by others within the School It’s also a function of availability of qualified postdocs who have enough expertise in each component area. There is also a general interest in having additional supporting documentation on how the samples were processed along with any and all details relating to quality control. The following figure shows a typical work flow based on a gut microbiome sample. The customary approach has been to accomplish Steps 2-3 (sometimes 1 also) via a trusted service core facility with steps 1, 4 and 5 being performed locally within the school by faculty, students, professional staff, collaborators and/or a combination thereof. Some faculty have leveraged personal relationships and employed external labs to accomplish all steps simply to expedite project completion although being able to conduct the analysis locally and within a reasonable time frame is a goal. Figure 4.1: Typical Microbiome Workflow If the analysis steps are broken down into the component tools (e.g. Qiime, DADA2, Mothur, etc) there is wide variation in software knowledge on how to best implement the software to arrive at a biologically relevant result. Additional experimentation with various software tools and pipelines workflow will occur although some tools require more substantial computation environments such as the Amazon cloud which itself can present a steep learning curve for investigators. After receiving initial results from the Core, investigators will frequently reengage with followup questions to better understand the result. Not all Cores have the resident domain expertise (or interest) to sustain and ongoing followup dialogue or, if they do, staff might not currently be available. 4.2 Understanding Results The following figure captures the essence of the challenges when interacting with Core facilities. The best situation occurs when there is an intersection of knowledge between the facility and the investigator to the point wherein questions and concerns are easily addressed. Both sides have a sufficient background to ask pointed questions about software pipelines used in the process as well as some knowledge of the domain from which the data was generated (e.g. Microbiome, Metabolomics, Lipidomics). The second case represents the case wherein the investigator has some knowledge of at least one of the areas but needs significantly more help in other areas. This situation is more common than the first. There is light intersection of shared knowledge which subsequently provokes more questions (sometimes more advanced, sometimes basic and remedial). 4.3 Need for More Consultation Additional consultation from the Cores is required given that researchers might not be well trained in a specific technique or field. This is true for both study and experimental design as well as result interpretation. Additionally, being able to point to vital working relationships to local Cores is very helpful when applying for grants. In absence of that type of relationship then external providers must be considered. One problem with core facilities is the recruitment and retention of the necessary domain expertise to complete a number of projects on time. Typically, core facilities are staffed with junior level faculty who are on their way to other career opportunities or staff who are working (many times temporarily) for less than market salaries perhaps because a partner or spouse is employed elsewhere within the institution. Lack of advancement and funding for professional development can dissuade otherwise qualified personnel from pursuing such positions. Universities such as Vanderbilt have begun to establish job descriptions and promotion and tenure policies relevant to the issue of career tracks for core directors and personnel. 4.4 Possible Solutions In response to faculty input the following figure represents a type of interaction with internal cores that can help. It assumes that supplemental expertise can be assigned or mobilized in parallel (to the extent that can currently be achieved) to fully leverage results coming back from a Core. Some investigators might already have this relationship in some form (e.g. postdocs, junior faculty working on the project) but it’s not a common scenario nor is it one that is realistic for junior faculty. The following figure represents a distillation of input from faculty wherein there is ready access to a group of integrated professionals who can manage interactions (where requested) and fulfill some of the needs that is typically pushed back onto the investigator. In this scenario investigators still retain the ability to go directly to a Core or leverage the strengths of the internal Core who can manage relationships, organize training, and intelligently allocate personnel for various tasks. Obviously, this will involve recruitment and possible reorganization of existing resources (those who aren’t already fully funded). "],
["project-initiation-dynamics.html", "Chapter 5 Project Initiation Dynamics 5.1 Continuity Throughout the Research Process 5.2 Estimating Effort 5.3 Grants", " Chapter 5 Project Initiation Dynamics Summary: People are generally happy with the project initiation process (PIF) as it alerts the organization to an impending proposal. Junior faculty appreciate the assistance and perspective being offered by senior researchers in this process and would like to see this continue. Where things aren’t so good relate to the availability of statistics, analytics, and computation resources. If one does not have funding, then this becomes a problem. Others feel that the more experienced technical personnel are not available because their time has been “bought out” by more senior personnel. 5.1 Continuity Throughout the Research Process Having access to analytics, statistics, computation, and biological expertise is important though being able to have simultaneous access to each of these professionals (assuming they are separate individuals) would be useful since no on individual has the “whole picture” and must frequently defer to another before being able to proceed past a roadblock. Initial consultations tend to center around design and general impressions about how to proceed though once the work starts, questions naturally emerge in response to intermediate results that might require input from a number of people. This is particularly true when investigators are leveraging their network and/or past professional relationships in other departments or institutions. They do this because of lack of funding to adequately compensate existing staff unless of course funding can be found within K grants. 5.2 Estimating Effort Investigators trust that self-declared experts and collaborators are providing reasonable effort estimates and, more importantly, that they will provide analysis support (or someone from their lab will) although this hasn’t always panned out in a way that was expected. Consequently, effort has been obtained from whatever available sources possible. The PIF approach does help to initiate the research process and alerts the organization to an impending grant application. The advice one receives as well is generally good though the staff in place for Omics support may not be available for the project or have the current skills to address the issues. 5.3 Grants Career Development Awards (K grants) are plentiful and many junior faculty feel that there is adequate support and mentorship on how to write proposals at least at that level. The perceived problem is that of generating the necessary preliminary data for obtaining an R series grant. Faculty point to difficulties in getting good results from the recent various Microbiome and Metabolomics projects. There is also a concern that pursuing grants of larger scope will be challenging given that the it is difficult to realistically reference formal means of analytics and computation support within proposals. "],
["computation.html", "Chapter 6 Computation 6.1 Data Management and Computation 6.2 Cloud vs Local Resources 6.3 Desktop Support for Open Source Analysis Tools 6.4 General IT Support 6.5 Databases and Applications", " Chapter 6 Computation Summary: Computation underlies research analytics though no official resource exists within the school, which can be challenging. Faculty use whatever resources they can find and will leverage relationships (personal and professional) to work around the issue. Lots of code and data winds up on laptops which works against reproducibility. Computation is not something that was directly mentioned by most faculty simply because it is generally considered to be an underlying component to data management and analysis and is therefore generally not the first topic referenced when discussing challenges. However, everyone understands that analysis and data manipulation must occur “somewhere” which is why some faculty leverage personal and professional relationships (Math/CS, Georgia Tech, Winship Cancer Institute) to accomplish the computation. Unfortunately, these resources aren’t necessarily available to other School of Nursing Faculty or if they are now, might not be if the SON person were to leave. 6.1 Data Management and Computation Smaller projects can in fact be accomplished using laptops and desktops though the trend has rapidly moved towards more samples per project thus being able to easily and conveniently compute against data sources is very important. This is a major limitation of Emory Box in that the data cannot be directly accessed via software. It might be more convenient for it to be in Amazon S3 bucket or a folder attached to a compute node, or in a system such as DNANexus or Seven Bridges both of which are enterprise frameworks for data management, analysis, and computation. These are currently being considered by WCI in support of their ORIEN http://oriencancer.org/#about project though it is quite possible that the School of Nursing could benefit from using the license also since it would provide a standard tool for management, analysis, and compute. However, embracing these tools would still involve formal training and orientation to ensure productivity. 6.2 Cloud vs Local Resources All of those interviewed are aware of the importance of resources such as Amazon or Google but lack the training to reasonably approach these resources directly unless “chaperoned” by someone more knowledgeable or by involving a funded collaborator. A minority of those interviewed express an interest in knowing how to configure cloud resources at a “nuts and bolts” level though most do not see immediate value in that. Although they do understand that being more facile at the UNIX command line and with R and Python can only help them. In the end, faculty are not fixated on a specific architecture or method of computation as long as it is readily available and accessible using standard, documented methods. Many faculty are getting their computational work done “by hook or by crook” so they are open to something more generally accessible that is setup according to a standard and doesn’t require them to “jump around” to different resources throughout the course of the project. On boarding To the Cloud is a problem. The cloud’s ability to match the scope of most any research project is indeed a true convenience which, however, requires significant knowledge to fully exploit. Knowing how to responsibly provision storage, “spin up” instances, create databases, and effectively close out projects are important skills. However, most investigators aren’t interested in doing it themselves (at least for the most part) and will need proxies to handle the foundational work which typically involves students who will depart at some point. This leaves everyone with a vague sense of having “gotten something done” but no one person seems to remember exactly how it was done, where the data or latest version of the pipeline is, or how to reproduce it. Most projects are much more than a mere “one off” execution of a pipeline. Research evolves over time and involves input from collaborators (statisticians, analysts, bioinformatics etc.) which means that the data must remain resident on the cloud as the project evolves. This ongoing occupancy involves costs and while someone could move the data off to cheaper storage and then move it back to EBS, this is inconvenient and also interrupts the continuity. Local resources are catch as catch can (frequently by personal arrangements) but charges don’t accrue although if the person leaves then access to the data and resource, along with a description thereof go missing. 6.3 Desktop Support for Open Source Analysis Tools Faculty have expressed concern at the level of IT desktop support they receive which is limited when considering that they need to conduct analyses on their desktops using open source tools. Being able to install and update R AND troubleshoot package installations is a common need although it has been difficult to get that type of help. While faculty would gladly use a more fully configured computation resource that is specifically customized for analysis and data manipulation there will always be a need for desktop computation simply to try things out and/or do trial runs of pipelines found in literature. To this end having someone in IT who is willing to do this and learn about the problems facing a researcher would be helpful. Note that faculty are NOT expecting the IT group to provide research advice or debug programming issues. However, they should be able to troubleshoot basic installation problems and be able to assist in the updating of packages. 6.4 General IT Support The focus of the interview process was primarily to collect impressions about the research process with a general focus on data management, analytics, and related issues. However, it wasn’t surprising that discussions about IT emerged as many of the faculty weren’t/aren’t entirely clear on the role that group plays in research matters if any. Generally, expectations are low about what IT can do though it is also agreed that this should not necessarily be acceptable - especially if they are being compensated as a group to assist faculty. Just because the group has not been motivated in the past to help researchers should not then mean that the group should not perhaps be reconfigured in some form to be more helpful. This could be useful in the sense that a reconfigured group could maintain local servers and databases but, again, this assumes a commitment to a new direction that would potentially require new (or retrained) personnel. Of course, there is an awareness that IT as a group is receiving some level of attention at this point in terms of a possible reorgnization. 6.5 Databases and Applications There were limited references to developing applications and/or web-enabled databases. Some faculty would want to do this in support of a research project though by far the more pressing need relates to analytics, statistics, and omics. Deploying web-enabled apps and database is actually a very common interest in research departments across the institution. Unfortunately, there is no common Emory service for this (though there should be) which then means that apps must be locally developed and maintained on local servers. Obviously, Amazon could be used as a foundation for apps, but ongoing cost is a concern. Still, investigators would pay if the resource were well supported, backed up, and maintained. In absence of this, the School would have to provide server space (backups, patches, upgrades, security auditing) as well as DBA (Database Administration) support for app hosting which in turn would require administrators. These administrators would NOT necessarily have to have research or analytics skills so if the IT group were reconfigured to provide more services then this might be one of them. "]
]
